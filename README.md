# ğŸ§  BERT avec TensorFlow Hub â€“ Guide Complet

[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?logo=tensorflow)](https://www.tensorflow.org/)
[![Python](https://img.shields.io/badge/Python-3.7+-3776AB?logo=python)](https://www.python.org/)
[![BERT](https://img.shields.io/badge/BERT-Transformer-green)](https://arxiv.org/abs/1810.04805)

> Guide dÃ©taillÃ© sur BERT (Bidirectional Encoder Representations from Transformers) avec implÃ©mentation TensorFlow Hub

---

## ğŸ“š Table des MatiÃ¨res

- [Introduction](#-introduction)
- [Architecture Transformer](#-architecture-transformer)
- [Concepts Fondamentaux](#-concepts-fondamentaux)
- [Composants de BERT](#-composants-de-bert)
- [PrÃ©-entraÃ®nement](#-prÃ©-entraÃ®nement)
- [Code & ImplÃ©mentation](#-code--implÃ©mentation)
- [Applications](#-applications)
- [Ressources](#-ressources)

---

## ğŸ¯ Introduction

### Qu'est-ce que BERT ?

**BERT (Bidirectional Encoder Representations from Transformers)** est un modÃ¨le rÃ©volutionnaire de traitement du langage naturel dÃ©veloppÃ© par Google AI en 2018.

#### âœ¨ CaractÃ©ristiques principales

| CaractÃ©ristique | Description |
|-----------------|-------------|
| **Bidirectionnel** | Lit le texte dans les deux sens (â†â†’) |
| **Contextuel** | Comprend le sens selon le contexte |
| **PrÃ©-entraÃ®nÃ©** | EntraÃ®nÃ© sur 3,3 milliards de mots |
| **Transfer Learning** | Adaptable Ã  diverses tÃ¢ches NLP |

#### ğŸ’¡ Exemple de comprÃ©hension contextuelle

```text
Phrase 1: "Je vais Ã  la banque pour dÃ©poser de l'argent"
         â†’ BERT comprend: banque = institution financiÃ¨re

Phrase 2: "Je m'assieds sur la banque du parc"
         â†’ BERT comprend: banque = siÃ¨ge

âœ… MÃªme mot, sens diffÃ©rent selon le contexte !
```

---

## ğŸ—ï¸ Architecture Transformer

### Le Transformer : Fondation de BERT

BERT utilise uniquement la partie **Encoder** du Transformer original (Vaswani et al., 2017).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TRANSFORMER ORIGINAL            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ENCODER        â”‚      DECODER       â”‚
â”‚  (utilisÃ© par      â”‚   (non utilisÃ©     â”‚
â”‚     BERT)          â”‚    par BERT)       â”‚
â”‚                    â”‚                    â”‚
â”‚  â€¢ Bidirectionnel  â”‚  â€¢ Unidirectionnel â”‚
â”‚  â€¢ Comprend        â”‚  â€¢ GÃ©nÃ¨re          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pourquoi uniquement l'Encoder ?

- **Encoder** : Comprend et analyse le texte (bidirectionnel)
- **Decoder** : GÃ©nÃ¨re du nouveau texte (unidirectionnel)

**BERT se concentre sur la comprÃ©hension, pas la gÃ©nÃ©ration.**

---

## ğŸ”¬ Concepts Fondamentaux

### 1ï¸âƒ£ Embeddings

Les embeddings transforment les mots en vecteurs numÃ©riques de 768 dimensions.

#### Types d'embeddings dans BERT

```python
Embedding Final = Token Embedding + Position Embedding + Segment Embedding
```

#### ğŸ“Š DÃ©tails de chaque embedding

**A. Token Embedding** (768 dimensions)

```python
"python" â†’ [0.23, -0.45, 0.12, 0.67, ..., 0.89]  # 768 valeurs
```

**B. Position Embedding**

```python
Position 0: [0.1, 0.2, 0.3, ..., 0.8]
Position 1: [0.3, 0.4, 0.5, ..., 0.9]
Position 2: [0.5, 0.6, 0.7, ..., 1.0]
```

â†’ Indique la position du mot dans la phrase (ordre des mots)

**C. Segment Embedding**

```python
Phrase A: [0, 0, 0, 0, 0, ...]
Phrase B: [1, 1, 1, 1, 1, ...]
```

â†’ Distingue diffÃ©rentes phrases dans l'input

#### ğŸ¨ Visualisation

```
Input: "Hello World"

Token Emb:    [0.2, 0.3, ...] + [0.5, 0.6, ...]
Position Emb: [0.1, 0.1, ...] + [0.2, 0.2, ...]
Segment Emb:  [0.0, 0.0, ...] + [0.0, 0.0, ...]
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Emb:    [0.3, 0.4, ...] + [0.7, 0.8, ...]
              
              "Hello"           "World"
```

---

### 2ï¸âƒ£ Self-Attention Mechanism

Le mÃ©canisme clÃ© qui permet Ã  BERT de comprendre le contexte.

#### ğŸ“ Formule mathÃ©matique

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

OÃ¹:
Q = Query  (requÃªte)   - "Que cherche-t-on ?"
K = Key    (clÃ©)       - "Quelles informations avons-nous ?"
V = Value  (valeur)    - "Quelles sont les informations ?"
d_k = dimension des clÃ©s (64 pour BERT)
```

#### ğŸ” Exemple concret

```text
Phrase: "Le chat mange la souris"

Self-Attention sur le mot "mange":

Chat    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.80  â† Forte attention (sujet)
Le      â–ˆâ–ˆâ–ˆâ–ˆ 0.10
mange   â–ˆâ–ˆ 0.05
la      â–ˆ 0.02
souris  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.30  â† Attention modÃ©rÃ©e (objet)

â†’ BERT comprend que "chat" est le sujet de "mange"
â†’ BERT comprend que "souris" est l'objet de "mange"
```

---

### 3ï¸âƒ£ Multi-Head Attention

BERT utilise **12 tÃªtes d'attention** en parallÃ¨le.

#### ğŸ¯ Pourquoi plusieurs tÃªtes ?

Chaque tÃªte apprend diffÃ©rents aspects linguistiques :

| TÃªte | Apprentissage |
|------|---------------|
| **TÃªte 1** | Relations syntaxiques (sujet-verbe) |
| **TÃªte 2** | Relations sÃ©mantiques (synonymes) |
| **TÃªte 3** | DÃ©pendances longues distances |
| **TÃªte 4** | CorÃ©fÃ©rences (pronoms â†’ noms) |
| **TÃªte 5-12** | Autres patterns linguistiques |

#### ğŸ“Š Architecture

```
                    Input
                      â†“
    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚Head1â”‚Head2â”‚Head3â”‚Head4â”‚...â”‚Head12â”‚
    â”‚     â”‚     â”‚     â”‚     â”‚   â”‚     â”‚
    â”‚ 64d â”‚ 64d â”‚ 64d â”‚ 64d â”‚...â”‚ 64d â”‚
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
      â†“     â†“     â†“     â†“         â†“
      â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
           Concatenation
          (12 Ã— 64 = 768)
                  â†“
              Output
```

---

### 4ï¸âƒ£ Feed-Forward Network

AprÃ¨s l'attention, chaque token passe par un rÃ©seau neuronal Ã  deux couches.

```
Input (768) â†’ Dense(3072) â†’ GELU â†’ Dense(768) â†’ Output

Expansion 4x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€ Compression
```

#### âš¡ Fonction d'activation GELU

```python
GELU(x) = x Ã— Î¦(x)  # Î¦ = fonction de distribution normale

Plus douce que ReLU, mieux adaptÃ©e au NLP
```

---

### 5ï¸âƒ£ Layer Normalization & Residual Connections

Architecture d'une couche Encoder complÃ¨te :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input                         â”‚
â”‚    â†“                           â”‚
â”‚  Multi-Head Attention          â”‚
â”‚    â†“                           â”‚
â”‚  Add & Normalize  â†â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â†“                      â”‚    â”‚
â”‚  Feed Forward Network     â”‚    â”‚
â”‚    â†“                      â”‚    â”‚
â”‚  Add & Normalize  â†â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚    â†“                           â”‚
â”‚  Output                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Residual Connections (â†’) Ã©vitent le gradient vanishing
```

---

## ğŸ§© Composants de BERT

### Configuration BERT-Base

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BERT-Base (utilisÃ© ici)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L = 12   Layers (Encoders)       â”‚
â”‚ H = 768  Hidden size (dimensions)â”‚
â”‚ A = 12   Attention heads         â”‚
â”‚ Params = 110 millions            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration BERT-Large

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BERT-Large               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L = 24   Layers                  â”‚
â”‚ H = 1024 Hidden size             â”‚
â”‚ A = 16   Attention heads         â”‚
â”‚ Params = 340 millions            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ·ï¸ Tokens SpÃ©ciaux

| Token | Nom | RÃ´le | Position |
|-------|-----|------|----------|
| `[CLS]` | Classification | ReprÃ©sente toute la phrase | DÃ©but |
| `[SEP]` | Separator | SÃ©pare deux phrases | Entre/Fin |
| `[PAD]` | Padding | Remplit pour Ã©galiser la longueur | Fin |
| `[MASK]` | Mask | Mot masquÃ© (entraÃ®nement) | Variable |
| `[UNK]` | Unknown | Mot hors vocabulaire | Variable |

#### ğŸ“ Exemple complet

```text
EntrÃ©e brute: "Python is great"

AprÃ¨s tokenisation:
[CLS] python is great [SEP] [PAD] [PAD] [PAD]
  â†‘      â†‘            â†‘      â†‘
  â”‚      â”‚            â”‚      â””â”€ SÃ©parateur
  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         (marqueur de fin)
  â””â”€ Token de classification
     (reprÃ©sentation globale)
```

---

### ğŸ“– Vocabulaire WordPiece

BERT utilise **WordPiece tokenization** avec **30 000 tokens**.

#### ğŸ”ª DÃ©coupage des mots

```text
Mot inconnu: "unbreakable"

Tokenization WordPiece:
["un", "##break", "##able"]
 â†‘      â†‘          â†‘
 â”‚      â”‚          â””â”€ Suffixe (## = continuation)
 â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Racine (continuation)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©fixe

Avantages:
âœ… GÃ¨re les mots rares
âœ… RÃ©duit la taille du vocabulaire
âœ… Partage les sous-mots communs
```

---

## ğŸ“ PrÃ©-entraÃ®nement

BERT est prÃ©-entraÃ®nÃ© sur **deux tÃ¢ches non supervisÃ©es** :

### 1ï¸âƒ£ Masked Language Model (MLM)

**Objectif** : PrÃ©dire les mots masquÃ©s dans une phrase.

#### ğŸ­ Processus

```text
1. Prendre une phrase
2. Masquer 15% des tokens alÃ©atoirement
3. BERT prÃ©dit les mots originaux

Exemple:

Original:   I love python programming
            â†“
MasquÃ©:     I love [MASK] programming
            â†“
PrÃ©diction: I love python programming âœ…
```

#### ğŸ“Š StratÃ©gie de masquage (pour les 15% choisis)

| Action | ProbabilitÃ© | Exemple |
|--------|-------------|---------|
| Remplacer par `[MASK]` | 80% | `I love [MASK]` |
| Remplacer par mot alÃ©atoire | 10% | `I love banana` |
| Laisser inchangÃ© | 10% | `I love python` |

**Pourquoi cette stratÃ©gie ?**

```text
80% [MASK] : EntraÃ®nement principal
10% alÃ©atoire : Ã‰vite de trop dÃ©pendre de [MASK]
10% inchangÃ© : Apprend les reprÃ©sentations sans masque
```

---

### 2ï¸âƒ£ Next Sentence Prediction (NSP)

**Objectif** : PrÃ©dire si la phrase B suit logiquement la phrase A.

#### ğŸ’¬ Exemples

**âœ… IsNext (label = 1)**

```text
Phrase A: "Je vais au marchÃ©"
Phrase B: "J'achÃ¨te des fruits"

â†’ Ces phrases se suivent logiquement
```

**âŒ NotNext (label = 0)**

```text
Phrase A: "Je vais au marchÃ©"
Phrase B: "Les Ã©toiles brillent la nuit"

â†’ Aucun lien logique entre les phrases
```

#### ğŸ“‹ Format d'entrÃ©e

```text
[CLS] Phrase A [SEP] Phrase B [SEP]
  â†‘            â†‘              â†‘
  â”‚            â”‚              â””â”€ Fin de B
  â”‚            â””â”€ SÃ©parateur A/B
  â””â”€ Classification (IsNext ou NotNext)
```

---

### ğŸ“š DonnÃ©es d'entraÃ®nement

| Source | Nombre de mots |
|--------|----------------|
| BooksCorpus | 800 millions |
| Wikipedia (EN) | 2,5 milliards |
| **Total** | **3,3 milliards** |

**Temps d'entraÃ®nement** : 4 jours sur 64 TPU v3

---

## ğŸ’» Code & ImplÃ©mentation

### ğŸ“¦ Installation

```bash
pip install tensorflow tensorflow-hub tensorflow-text
```

### ğŸ”§ Imports

```python
import tensorflow_hub as hub
import tensorflow_text as text
```

**RÃ´le des bibliothÃ¨ques :**

| BibliothÃ¨que | Fonction |
|--------------|----------|
| `tensorflow_hub` | Charge modÃ¨les prÃ©-entraÃ®nÃ©s |
| `tensorflow_text` | OpÃ©rations NLP (tokenisation) |

---

### ğŸŒ URLs des modÃ¨les

```python
preprocess_url = "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
encoder_url = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"
```

#### ğŸ” DÃ©composition de l'URL encoder

```text
bert_en_uncased_L-12_H-768_A-12
â”‚    â”‚  â”‚        â”‚    â”‚     â”‚
â”‚    â”‚  â”‚        â”‚    â”‚     â””â”€ A-12  : 12 Attention heads
â”‚    â”‚  â”‚        â”‚    â””â”€â”€â”€â”€â”€â”€â”€ H-768 : 768 dimensions cachÃ©es
â”‚    â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ L-12  : 12 couches Encoders
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ uncased : minuscules uniquement
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ en : anglais
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ bert : modÃ¨le BERT
```

---

### ğŸ”„ Chargement du prÃ©processeur

```python
bert_preprocess_model = hub.KerasLayer(preprocess_url)
```

**Ce que fait le prÃ©processeur :**

```
Texte brut
    â†“
1. Tokenisation (WordPiece)
    â†“
2. Conversion en IDs numÃ©riques
    â†“
3. Ajout tokens [CLS], [SEP]
    â†“
4. GÃ©nÃ©ration masques et segments
    â†“
DonnÃ©es prÃªtes pour BERT
```

---

### ğŸ“ DonnÃ©es d'entrÃ©e

```python
text_test = ['nice movie indeed', 'I love python programming']
```

**Format :**
- Liste de chaÃ®nes de caractÃ¨res
- Batch de 2 phrases
- Pas de prÃ©traitement manuel nÃ©cessaire

---

### âš™ï¸ PrÃ©traitement

```python
text_preprocessed = bert_preprocess_model(text_test)
print(text_preprocessed.keys())
```

**Sortie :**

```python
dict_keys(['input_word_ids', 'input_mask', 'input_type_ids'])
```

#### ğŸ” Visualisation dÃ©taillÃ©e

```text
Phrase: "nice movie indeed"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Token        â”‚ [CLS]â”‚ nice  â”‚ movie â”‚indeed â”‚ [SEP]â”‚ [PAD]â”‚ [PAD]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚input_word_idsâ”‚ 101  â”‚ 3835  â”‚ 3185  â”‚ 5442  â”‚ 102  â”‚  0   â”‚  0   â”‚
â”‚input_mask    â”‚  1   â”‚  1    â”‚  1    â”‚  1    â”‚  1   â”‚  0   â”‚  0   â”‚
â”‚input_type_idsâ”‚  0   â”‚  0    â”‚  0    â”‚  0    â”‚  0   â”‚  0   â”‚  0   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
                 â†‘      â†‘       â†‘       â†‘       â†‘      â†‘      â†‘
                 â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”˜
            Token de                Vrais tokens        Padding
          classification
```

---

### ğŸ”¢ input_word_ids

```python
print(text_preprocessed['input_word_ids'])
```

**Exemple de sortie :**

```python
array([[  101,  3835,  3185,  5442,   102,     0,     0],
       [  101,  1045,  2293, 15894,  4730,   102,     0]])
```

**Explication :**

```python
Phrase 1: "nice movie indeed"
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ 101  â”‚ 3835  â”‚ 3185  â”‚ 5442  â”‚ 102  â”‚  0   â”‚  0   â”‚
â””â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”˜
   â”‚       â”‚       â”‚       â”‚      â”‚      â”‚      â”‚
  [CLS]  nice   movie  indeed  [SEP] [PAD] [PAD]
```

**IDs spÃ©ciaux :**
- `101` = `[CLS]`
- `102` = `[SEP]`
- `0` = `[PAD]`

---

### ğŸ­ input_mask

```python
print(text_preprocessed['input_mask'])
```

**Sortie :**

```python
array([[1, 1, 1, 1, 1, 0, 0],
       [1, 1, 1, 1, 1, 1, 0]])
```

**RÃ´le :**

```
1 = Token rÃ©el (BERT traite)
0 = Padding (BERT ignore)

Phrase 1: [1, 1, 1, 1, 1, 0, 0]
           â†‘  â†‘  â†‘  â†‘  â†‘  â†‘  â†‘
           â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”´â”€ Padding (ignorÃ©)
           â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜
           Tokens rÃ©els (traitÃ©s)
```

**Importance :** Ã‰vite que BERT n'apprenne des patterns sur le padding.

---

### ğŸ·ï¸ input_type_ids

```python
print(text_preprocessed['input_type_ids'])
```

**Sortie :**

```python
array([[0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0]])
```

**RÃ´le : Distinguer les segments**

```text
Cas avec deux phrases :

Input: "Hello world [SEP] How are you [SEP]"

input_type_ids:
[0, 0, 0, 1, 1, 1, 1]
 â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Phrase A   Phrase B
```

**Dans notre cas** (une seule phrase) :
- Tous les `input_type_ids` = `0`

**Utilisation :**
- Question Answering (question + contexte)
- SimilaritÃ© de phrases
- InfÃ©rence de langage naturel (NLI)

---

### ğŸ§  Chargement de l'encodeur BERT

```python
bert_model = hub.KerasLayer(encoder_url)
```

**Contenu du modÃ¨le :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Encodeur BERT        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 12 couches Encoder   â”‚
â”‚ â€¢ Multi-Head Attention â”‚
â”‚ â€¢ Feed-Forward Networksâ”‚
â”‚ â€¢ Layer Normalization  â”‚
â”‚ â€¢ 110M paramÃ¨tres      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸš€ Passage dans BERT

```python
bert_results = bert_model(text_preprocessed)
print(bert_results.keys())
```

**Sortie :**

```python
dict_keys(['pooled_output', 'sequence_output', 'encoder_outputs', 'default'])
```

#### ğŸ“Š Vue d'ensemble des sorties

```
text_preprocessed
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT Encoder â”‚
â”‚   (12 layers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        â”œâ”€â†’ sequence_output  (tokens individuels)
        â”œâ”€â†’ pooled_output    (phrase entiÃ¨re)
        â””â”€â†’ encoder_outputs  (couches intermÃ©diaires)
```

---

### ğŸ“¤ sequence_output

```python
print(bert_results['sequence_output'])
print(bert_results['sequence_output'].shape)
```

**Dimensions :** `(batch_size, sequence_length, 768)`

```
Exemple: (2, 7, 768)
         â”‚  â”‚  â””â”€â”€ 768 dimensions d'embedding
         â”‚  â””â”€â”€â”€â”€â”€ 7 tokens par phrase
         â””â”€â”€â”€â”€â”€â”€â”€â”€ 2 phrases dans le batch
```

#### ğŸ¯ Contenu

```python
Phrase: [CLS] nice movie indeed [SEP] [PAD] [PAD]

Embeddings:
[CLS]    â†’ [0.12, -0.34, 0.56, ..., 0.89]  # 768 valeurs
nice     â†’ [0.23, 0.45, -0.12, ..., 0.34]  # 768 valeurs
movie    â†’ [-0.56, 0.78, 0.23, ..., -0.45] # 768 valeurs
indeed   â†’ [0.67, -0.23, 0.91, ..., 0.12]  # 768 valeurs
[SEP]    â†’ [0.34, 0.12, -0.67, ..., 0.78]  # 768 valeurs
[PAD]    â†’ [0.00, 0.00, 0.00, ..., 0.00]  # 768 valeurs
[PAD]    â†’ [0.00, 0.00, 0.00, ..., 0.00]  # 768 valeurs
```

#### ğŸ’¼ Utilisation

| TÃ¢che | Description |
|-------|-------------|
| **NER** | Named Entity Recognition |
| **POS Tagging** | Part-of-Speech tagging |
| **Question Answering** | Trouver rÃ©ponse dans texte |
| **Token Classification** | Classifier chaque mot |

**Exemple d'utilisation :**

```python
# Extraire l'embedding du 2Ã¨me mot (nice)
word_embedding = bert_results['sequence_output'][0, 1, :]
print(word_embedding.shape)  # (768,)
```

---

### ğŸ¯ pooled_output

```python
print(bert_results['pooled_output'])
print(bert_results['pooled_output'].shape)
```

**Dimensions :** `(batch_size, 768)`

```
Exemple: (2, 768)
         â”‚  â””â”€â”€ 768 dimensions
         â””â”€â”€â”€â”€â”€ 2 phrases
```

#### ğŸ§® Calcul

```python
pooled_output = tanh(Dense(sequence_output[CLS]))

Ã‰tapes:
1. Prendre l'embedding du token [CLS]
2. Passer par une couche Dense
3. Appliquer activation tanh
4. Obtenir reprÃ©sentation de la phrase
```

#### ğŸ¨ Visualisation

```
Phrase complÃ¨te: "nice movie indeed"
                        â†“
              Passe par BERT (12 layers)
                        â†“
         [CLS] nice movie indeed [SEP]
           â†“
    Embedding [CLS] (768)
           â†“
     Dense Layer
           â†“
    Activation tanh
           â†“
   pooled_output (768)
           â†“
   ReprÃ©sentation de toute la phrase
```

#### ğŸ’¼ Utilisation

| TÃ¢che | Description |
|-------|-------------|
| **Classification de texte** | Positif/NÃ©gatif/Neutre |
| **Analyse de sentiment** | Ã‰motions |
| **SimilaritÃ© sÃ©mantique** | Comparer phrases |
| **Classification binaire** | Spam/Ham, etc. |

**Exemple d'utilisation :**

```python
# Classifier le sentiment
from tensorflow.keras import layers

classifier = tf.keras.Sequential([
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(3, activation='softmax')  # 3 classes: pos/neg/neu
])

sentiment_logits = classifier(bert_results['pooled_output'])
```

---

### ğŸ” encoder_outputs

```python
print(len(bert_results['encoder_outputs']))
# Sortie: 12
```

**Contenu :**
- Liste des sorties de **chaque couche Encoder**
- 12 tenseurs de forme `(batch_size, seq_length, 768)`

#### ğŸ“š Structure

```python
encoder_outputs[0]  â†’ Sortie de la couche 1  (aprÃ¨s 1er Encoder)
encoder_outputs[1]  â†’ Sortie de la couche 2  (aprÃ¨s 2Ã¨me Encoder)
encoder_outputs[2]  â†’ Sortie de la couche 3
...
encoder_outputs[11] â†’ Sortie de la couche 12 (derniÃ¨re couche)
```

#### âœ… VÃ©rification

```python
# La derniÃ¨re couche Encoder = sequence_output
print(bert_results['encoder_outputs'][-1] == bert_results['sequence_output'])
# Sortie: True
```

#### ğŸ”¬ Analyse des couches

```python
# AccÃ©der Ã  une couche intermÃ©diaire
layer_6_output = bert_results['encoder_outputs'][5]
print(layer_6_output.shape)  # (2, 7, 768)
```

#### ğŸ’¼ Utilisation

| Usage | Description |
|-------|-------------|
| **Analyse linguistique** | Ã‰tudier ce que chaque couche apprend |
| **Probing tasks** | Tester la connaissance syntaxique/sÃ©mantique |
| **Feature extraction** | Combiner plusieurs couches |
| **Visualisation** | Voir l'Ã©volution des embeddings |

---

### ğŸŒŠ Pipeline Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE BERT COMPLET                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Texte brut
   "nice movie indeed"
        â†“

2. PrÃ©processeur BERT
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Tokenisation      â”‚
   â”‚ â€¢ WordPiece         â”‚
   â”‚ â€¢ Ajout [CLS], [SEP]â”‚
   â”‚ â€¢ GÃ©nÃ©ration masquesâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“

3. EntrÃ©es numÃ©riques
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ input_word_ids   â”‚ â†’ [101, 3835, 3185, 5442, 102, 0, 0]
   â”‚ input_mask       â”‚ â†’ [1, 1, 1, 1, 1, 0, 0]
   â”‚ input_type_ids   â”‚ â†’ [0, 0, 0, 0, 0, 0, 0]
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“

4. Embeddings
   Token + Position + Segment
        â†“

5. BERT Encoder (12 couches)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Couche 1         â”‚ â†’ encoder_outputs[0]
   â”‚ Couche 2         â”‚ â†’ encoder_outputs[1]
   â”‚ ...              â”‚
   â”‚ Couche 12        â”‚ â†’ encoder_outputs[11]
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“

6. Sorties finales
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ sequence_output  â”‚  pooled_output   â”‚ encoder_outputs  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€#   p r o j e c t  
 